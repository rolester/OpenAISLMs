{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(model_path=\"C:\\\\Phi-2\\\\phi-2.Q5_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=3584,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=4,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=30,         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    "  verbose=True)\n",
    "\n",
    "#if BLAS = 0 then not using GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: Hello, I'm glad to hear that you're having a good day. I'm doing well too, thank you for asking.\n",
      "\n",
      "CPU times: total: 5.5 s\n",
      "Wall time: 5.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a text prompt\n",
    "prompt = \"Hello I am having a great day. How are you doing?\"\n",
    "# generate a response (takes several seconds)\n",
    "output = llm(prompt,temperature=0.1, max_tokens=500)\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT SUM(Temp) FROM [DeviceType] WHERE DaviceTypeName = 'DaviceTypeName';\n",
      "\n",
      "CPU times: total: 1.14 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a text prompt\n",
    "prompt = \"\"\"Create a SQL statement to get the sum of temprature by DaviceTypeName from the following schema:\n",
    "\n",
    "[AItemp](\n",
    "\t[ID] [int] IDENTITY(1,1) NOT NULL,\n",
    "\t[Temp] [decimal](9, 4) NULL,\n",
    "\t[Hum] [decimal](9, 4) NULL,\n",
    "\t[timerec] [datetime2](7) NOT NULL,\n",
    "\t[Location] [varchar](150) NULL)\n",
    "\n",
    "[DeviceType](\n",
    "\t[AItemp] [int] NOT NULL,\n",
    "\t[DaviceTypeName] VARCHAR(150) NULL)\n",
    " \n",
    "\"\"\"\n",
    "# generate a response (takes several seconds)\n",
    "output = llm(prompt,temperature=0.1, max_tokens=500)\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: def sum_of_squares(numbers):\n",
      "  # initialize the sum to zero\n",
      "  total = 0\n",
      "  # loop through the list of numbers\n",
      "  for num in numbers:\n",
      "    # square each number and add it to the sum\n",
      "    total += num ** 2\n",
      "  # return the sum\n",
      "  return total\n",
      "\n",
      "CPU times: total: 9.23 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a text prompt\n",
    "prompt = \"\"\"Write a python function that takes a list of numbers and returns the sum of the squares of all the numbers. \n",
    "\"\"\"\n",
    "# generate a response (takes several seconds)\n",
    "output = llm(prompt,temperature=0.1, max_tokens=500)\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create a text prompt\n",
    "prompt = \"What are the names of the 7 days of the week?\"\n",
    "# generate a response (takes several seconds)\n",
    "output = llm(prompt,temperature=0.1, max_tokens=500)\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: You now have 11 apples.\n",
      "\n",
      "CPU times: total: 25.1 s\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a text prompt\n",
    "prompt = \"I have 10 apples - I make apple pie which uses 6 of the apples. I then buy 1 more apple. How many do I have?\"\n",
    "# generate a response (takes several seconds)\n",
    "output = llm(prompt,temperature=0.1, max_tokens=500)\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The odd one out is the fish because it is not an animal that can fly or make a sound like the other animals.\n",
      "\n",
      "CPU times: total: 19.4 s\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a text prompt\n",
    "prompt = \"Rat, Cat, Mouse, Dog, Spade, Fish, Fly, Raven. Which is the odd one out and why?\"\n",
    "# generate a response (takes several seconds)\n",
    "output = llm(prompt,temperature=0.1, max_tokens=500)\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
