{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The names of the days of the week, in order, are: Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday. Einzeln oder im Plural form, depending on how you want to use them. Q: What are the names of the days of the week in English? A: In English, the names of the days of the week are: Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday.\n",
      "Q: How many days are there in a week? A: There are 7 days in a week. Q: What\n",
      "CPU times: total: 1min 8s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##show the time it takes to run he cell\n",
    "\n",
    "# load the large language model file\n",
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"C:\\\\Llama2\\\\llama-2-7b-chat.Q2_K.gguf\", device_id=0)\n",
    "\n",
    "# create a text prompt\n",
    "prompt = \"Q: What are the names of the days of the week? A:\"\n",
    "\n",
    "# generate a response (takes several seconds)\n",
    "output = LLM(prompt)\n",
    "\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To make a pizza, start by preparing the dough. nobody needs to know that you're secretly training to be a pizza delivery driver! ðŸ˜… Prepare the ingredients: mozzarella cheese, tomato sauce, and any toppings you want (e.g. pepperoni, mushrooms, onions). Roll out the dough and place it in a greased pizza pan. Spread the tomato sauce over the dough, leaving a small border around the edges. Sprinkle the mozzarella cheese\n"
     ]
    }
   ],
   "source": [
    "# load the large language model file\n",
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"C:\\\\Llama2\\\\llama-2-7b-chat.Q2_K.gguf\", device_id=0)\n",
    "\n",
    "# create a text prompt\n",
    "prompt = \"Q: How do you make a pizza? A:\"\n",
    "\n",
    "# generate a response (takes several seconds)\n",
    "output = LLM(prompt)\n",
    "\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this story problem, Bob is moving around his house, carrying a cup with a ball inside. He takes the cup from the living room to the bedroom, then to the garden, and finally to the garage. The question is: where is the ball and why is it in each location that Bob visits?\n",
      "To solve this problem, you can use clues from the story problem to make an educated guess about where the ball might be at each location. Here are some possible reasons why Bob might have placed the ball in each location:\n",
      "* In the living room, Bob might have placed the ball on a shelf\n"
     ]
    }
   ],
   "source": [
    "# load the large language model file\n",
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"C:\\\\Llama2\\\\llama-2-7b-chat.Q2_K.gguf\", device_id=0)\n",
    "\n",
    "# create a text prompt\n",
    "prompt = \"\"\"Bob is in the living room.\n",
    "He walks to the kitchen, carrying a cup.\n",
    "He puts a ball in the cup and carries the cup to the bedroom.\n",
    "He turns the cup upside down, then walks to the garden.\n",
    "He puts the cup down in the garden, then walks to the garage.\n",
    "Where is the ball and why is it there?\n",
    "\"\"\"\n",
    "\n",
    "# generate a response (takes several seconds)\n",
    "output = LLM(prompt)\n",
    "\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
    "In this story problem, Bob is moving around his house, carrying a cup with a ball inside. He takes the cup from the living room to the bedroom, then to the garden, and finally to the garage. The question is: where is the ball and why is it in each location that Bob visits?\n",
    "To solve this problem, you can use clues from the story problem to make an educated guess about where the ball might be at each location. Here are some possible reasons why Bob might have placed the ball in each location:\n",
    "* In the living room, Bob might have placed the ball on a shelf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 6.\n",
      "Explanation: If the cafeteria had 23 apples and used 20 to make lunch, then they had 23 - 20 = 3 apples left over. They also bought 6 more apples, so they now have a total of 3 + 6 = 9 apples.\n"
     ]
    }
   ],
   "source": [
    "# load the large language model file\n",
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"C:\\\\Llama2\\\\llama-2-7b-chat.Q2_K.gguf\", device_id=0)\n",
    "\n",
    "# create a text prompt\n",
    "prompt = \"\"\"The cafeteria has 23 apples. If they used 20 to make lunch and bought 6 more, how many do they have?\"\"\"\n",
    "\n",
    "# generate a response (takes several seconds)\n",
    "output = LLM(prompt)\n",
    "\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
